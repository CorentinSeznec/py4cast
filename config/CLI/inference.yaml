
data:
  dataset: "poesy"
  num_input_steps: 2
  num_pred_steps_train: 4
  num_pred_steps_val_test: 8

  batch_size: 4
  num_workers: 10
  prefetch_factor: null
  pin_memory: False
  persistent_workers: False
  
  dataset_conf: null
  config_override: null

model:
  model_name: "halfunet"
  model_conf: null
  num_inter_steps: 1
  lr: 1e-4
  loss: "mse"
  training_strategy: "diff_ar"
  use_lr_scheduler: False
  precision: "32"

  no_log: False # doublon avec trainer.logger
  channels_last: False
  save_path: "/scratch/shared/py4cast/logs/test_cli"

trainer:
  fast_dev_run: False
  num_nodes: 1
  devices: "auto"
  max_epochs: 10
  deterministic: True
  strategy: "ddp"
  accumulate_grad_batches: 10
  accelerator: "auto"
  logger: 
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      # save_dir: "/scratch/shared/py4cast/logs/test_cli"
      name: "folder_logger"
      version: "beta"
      default_hp_metric: False
  profiler: null
  log_every_n_steps: 1
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        # dirpath: "/scratch/shared/py4cast/logs/test_cli"
        filename: "{epoch:02d}-{val_mean_loss:.2f}"
        monitor: "val_mean_loss"
        mode: "min"
        save_top_k: 1  # Save only the best model
        save_last: True  # Also save the last model
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: "val_mean_loss"
        mode: "min"
        patience: 50
    - class_path: py4cast.cli.CustomWriter
      init_args:
        write_interval: 'epoch'
        output_dir: "/scratch/shared/py4cast/gribs_writing/test_callback"


  check_val_every_n_epoch: 1
  precision: 32
  limit_train_batches: 5
  limit_val_batches: 5
  limit_test_batches: 5

